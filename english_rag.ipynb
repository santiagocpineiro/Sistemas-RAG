{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://text.npr.org/nx-s1-5227172'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Supongamos que `response.text` contiene el HTML de la p√°gina\n",
    "html_content = response.text\n",
    "\n",
    "# Crear un objeto BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraer todas las etiquetas <p>\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Obtener solo el texto de las etiquetas <p>\n",
    "paragraph_texts = [p.get_text(strip=True) for p in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=paragraph_texts,\n",
    "    collection_name=\"microplastics\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"/tmp/chroma_microplastics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='As they push to solve the puzzle of microplastics, here are six questions scientists are trying to answer.'\n",
      "page_content='Our soil, drinking water and food supply, the air we breathe, all carry microplastics,defined asany plastic particle as small as 1 nanometer and as large as 5 millimeters. Some have built up in the environment over many years, while others arrive daily, as they shed from tires, our clothing, food packaging, personal care products and more.'\n",
      "page_content='Humans encounter many pollutants over our lifetime. And given that researchers are still sorting out the best models for analyzing microplastics, many are cautious not to get ahead of the data.'\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What is a microplastic\",\n",
    "    k=3  #number of results\n",
    ")\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "def search_with_retriever(query, top_k):\n",
    "    try:\n",
    "        retriever.search_kwargs[\"k\"] = top_k  # Dynamically set the number of results\n",
    "        results = retriever.get_relevant_documents(query)\n",
    "        return \"\\n\\n\".join(\n",
    "            [f\"**Result {i+1}:**\\n{doc.page_content}\" for i, doc in enumerate(results)]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Local LLM\n",
    "ollama_llm = \"tinyllama\"\n",
    "model_local = ChatOllama(model=ollama_llm)\n",
    "\n",
    "# Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question asks for information about a character in the given context, Suzanne Brander.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is Brander?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### Chroma Database Search\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(label=\"Enter Your Query\", placeholder=\"Type your question here...\")\n",
    "\n",
    "    search_button = gr.Button(\"Search\")\n",
    "    output_box = gr.Textbox(label=\"Search Results\", lines=15)\n",
    "\n",
    "    # Bind the function to the Gradio UI\n",
    "    search_button.click(fn=chain.invoke, inputs=[query_input], outputs=output_box)\n",
    "\n",
    "# Launch the App\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
